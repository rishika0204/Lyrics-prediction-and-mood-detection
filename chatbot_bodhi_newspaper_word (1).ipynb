{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4p-bBk2tHzf",
        "outputId": "05184e44-ae85-4823-8212-e6deb76074bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install newspaper3k # an liabrary to extract the newspaper article , we can replace it and use it for song lyrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wBS3N0gteG3",
        "outputId": "27738c37-7e40-48ef-ffa6-940d4e555d63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/211.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/211.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (4.11.2)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (9.4.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (6.0.1)\n",
            "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (4.9.3)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (3.8.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.31.0)\n",
            "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
            "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tldextract>=2.0.1 (from newspaper3k)\n",
            "  Downloading tldextract-3.4.4-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.3/93.3 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting feedfinder2>=0.0.4 (from newspaper3k)\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.8.2)\n",
            "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
            "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (4.66.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (2023.7.22)\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
            "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.12.2)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13538 sha256=f332f7550d5e603bdcef5b9f7b91d802ae8d41653e2f8db3049dd7d46598b58d\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/d6/6c/384f58df48c00b9a31d638005143b5b3ac62c3d25fb1447f23\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3340 sha256=913207ba7bae59a9482a58500f1b111294b78b3b9952bd2dbf2df7ae1042e5c4\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/02/e7/a1ff1760e12bdbaab0ac824fae5c1bc933e41c4ccd6a8f8edb\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398381 sha256=02960d4abb6be2af7fb6bf07aa39b54a52e44a69556c1f75f2ab5397c73d4f8d\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/c4/0c/12a9a314ecac499456c4c3b2fcc2f635a3b45a39dfbd240299\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6048 sha256=ea5f594933e3a7166ce3aa32e406b2c475dace14f60f155660524aa730381e6f\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, feedparser, cssselect, requests-file, feedfinder2, tldextract, newspaper3k\n",
            "Successfully installed cssselect-1.2.0 feedfinder2-0.0.4 feedparser-6.0.10 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.5.1 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-3.4.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from newspaper import Article\n",
        "import random\n",
        "import string\n",
        "import numpy as np\n",
        "import warnings\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  #convert collection of raw documents and matrix of tf-idf features\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "0gzWtmOgtx6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings('ignore')  #ignore the warnings"
      ],
      "metadata": {
        "id": "GW8MYjSzt4nW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#download package from nltk\n",
        "nltk.download('punkt',quiet=True)\n",
        "nltk.download('wordnet',quiet=True)"
      ],
      "metadata": {
        "id": "m0LlkDCHuA46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7biOEi7PuCHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get artical url\n",
        "article= Article('https://en.wikipedia.org/wiki/Quantum_computing')# dowloading the site of wikipedia from where the article is to be read\n",
        "article.download()\n",
        "article.parse()\n",
        "article.nlp()\n",
        "corpus=article.text\n",
        "#print\n",
        "print(corpus)\n",
        "# here the article is being printed which we have taken , we can show the whole song lyrics from the api from where we are extracting the song list"
      ],
      "metadata": {
        "id": "JmRrWPIHuGeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=corpus\n",
        "sent_tokens=nltk.sent_tokenize(text)\n",
        "print(sent_tokens)"
      ],
      "metadata": {
        "id": "UbqoJ64juszp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remove_punct_dict=dict( (ord(punct),None) for punct in string.punctuation)  #stroing all the punctuations\n",
        "print(string.punctuation)\n",
        "print(remove_punct_dict)"
      ],
      "metadata": {
        "id": "3GI2F_WYuwM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def LemNormalize(text):\n",
        "  return nltk.word_tokenize(text.lower().translate(remove_punct_dict))  #all the words are in lower case\n",
        "print(LemNormalize(text))"
      ],
      "metadata": {
        "id": "DeyBpqkiu4Xw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "greeting_input=[\"hi\",\"hello\",\"hey\",\"hola\"]  #keywords for greetings\n",
        "greeting_response=[\"howdy\",\"hey there\",\"hi\",\"hello :)\"]\n",
        "def greeting(sentence):\n",
        "  for word in sentence.split():\n",
        "    if word.lower() in greeting_input:\n",
        "      return random.choice(greeting_response)"
      ],
      "metadata": {
        "id": "VuGpP08Zu9HT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "greeting_input=[\"hi\",\"hello\",\"hey\",\"hola\"]  #keywords for greetings\n",
        "greeting_response=[\"howdy\",\"hey there\",\"hi\",\"hello :)\"]\n",
        "def greeting(sentence):\n",
        "  for word in sentence.split():\n",
        "    if word.lower() in greeting_input:\n",
        "      return random.choice(greeting_response)"
      ],
      "metadata": {
        "id": "TeAos_CAvAAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jyD2DOA7vLUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY05X-aeOU4O"
      },
      "source": [
        "def response(user_response):\n",
        " #user response and robo responce\n",
        "  #user_response=\"WHat is chronic disease\"\n",
        "  user_response=user_response.lower()\n",
        "  #print(user_response)\n",
        "  #robo response\n",
        "  robo_response=''\n",
        "  sent_tokens.append(user_response)\n",
        "  #print(sent_tokens)\n",
        "  tfidfvec=TfidfVectorizer(tokenizer=LemNormalize , stop_words='english')\n",
        "  tfidf=tfidfvec.fit_transform(sent_tokens)\n",
        "  #print(tfidf)\n",
        "  #get similarity score\n",
        "  val=cosine_similarity(tfidf[-1],tfidf)\n",
        "  #print(val)\n",
        "  idx=val.argsort()[0][-2]\n",
        "  flat=val.flatten()\n",
        "  flat.sort()\n",
        "  score=flat[-2]\n",
        "  #print(score)\n",
        "  if score==0:\n",
        "    robo_response=robo_response+\"sorry,i dont understand\"\n",
        "  else:\n",
        "    robo_response=robo_response+sent_tokens[idx]\n",
        "\n",
        "  sent_tokens.remove(user_response)\n",
        "  return robo_response"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "greeting_input=[\"hi\",\"hello\",\"hey\",\"hola\",\"namaste\"]\n",
        "greeting_response=[\"howdy\",\"namaste🙏️\",\"hey there\",\"hi\",\"hello :)\"]\n",
        "def greeting(sentence):\n",
        "  for word in sentence.split():\n",
        "    if word.lower() in greeting_input:\n",
        "      return random.choice(greeting_response)\n",
        "flag=True\n",
        "print(\"hello!!! this is robo,I can answer your querys related to quantum computing,type bye to exit\")\n",
        "while(flag==True):\n",
        "  user_response=input(\"user:\")\n",
        "  #user_response=user_response.lower()\n",
        "  if(user_response!='bye'):\n",
        "    if(user_response=='thanks' or user_response=='thank you'):\n",
        "      flag=False\n",
        "      print(\"robo: anytime :)\")\n",
        "    else:\n",
        "       if( greeting(user_response) != None):\n",
        "         print(\"robo: \"+ greeting(user_response))\n",
        "       else:\n",
        "         print(\"robo:\"+response(user_response))\n",
        "  else:\n",
        "    flag=False\n",
        "    print(\"robo: see you later :)\")"
      ],
      "metadata": {
        "id": "YgSIAvrqvS58"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}